\section{Klassifizierung}
\label{Klassifizierung}

\subsection{Stand der Technik}
\label{Stand der Technik}

\subsection{Weka}
\label{Weka}
Weka ist ein bekanntes Framework f"ur maschinelles Lernen. Die in Java geschriebene Software wurde von der \textit{University of Waikato, New Zealand} entwickelt und unter \textit{GNU General Public License} ver"offentlicht. \\
Weka bietet viele, f"ur diese Arbeit wichtige, Features an. Insbesondere die integrierten Filter und die automatische Interpretation von Instanzen zur Klassifikation ersparen viel Arbeit. Weka implementiert eine Vielzahl von Klassifikatoren, darunter auch das Multilayer Perceptron (MLP).
Au{\ss}erdem bietet es einige Methoden zur Evaluation der erstellten Modelle wie zum Beispiel die k-fold-Evaluierung. Dabei ist es jedoch nicht auf die massiv parallele Verarbeitung moderner Prozessoren ausgelegt und ist deshalb recht langsam.\\
Diese Arbeit verwendet Wekas Implementierung des MLP wegen der einfachen Verwendung und trotzdem guten Einstellbarkeit des Netzwerks. Au{\ss}erdem verringern die bereits implementierten Funktionen zur Filterung und Evaluation den Aufwand f"ur die hier erstellte Software enorm.

\subsection{Parameter}
\label{Parameter}
Ein MLP hat einige wichtige Parameter, die zur Genauigkeit der Klassifikation beitragen. Dieses Kapitel soll aber lediglich auf die Parameter hinweisen, die besten besten Werte f"ur jeden Parameter wird das Kapitel der \textit{Evaluation} suchen. Mit dem ersten wichtigen Parameter hat bereits das vorherige Kapitel besch"aftigt: Die Features. Diese spielen in der Klassifikation eine zentrale Rolle und wurden daher gesondert behandelt.\\
Die Anzahl der Epochen (N) gibt an, wie oft das Netz die Trainingsinstanzen w"ahrend des Trainings durchl"auft. Bei jedem Durchlauf wird der vom Netz verursachte Fehler mit dem Backpropagation Algorithmus minimiert, ist die Anzahl der Epochen zu gering, kann der Fehler nicht ausreichend minimiert werden. \\
Die Geschwindigkeit, mit der die Gewichte innerhalb des Netzes angepasst und somit der Fehler minimiert wird, wird durch die Lernrate (L) bestimmt. Eine h"ohere Lernrate f"uhrt dazu, dass das Netz schneller in Richtung des lokalen Optimums konvergiert, kann aber auch dazu f"uhren, dass es das Optimum "uberspringt und es nie genau erreicht. Dies kann aber durchaus erw"unscht sein, wenn eine sehr niedrige Lernrate zu einem lokalen Optimum konvergiert, das deutlich unter dem, von einer h"oheren Lernrate gefundenen Maximum liegt. Au{\ss}erdem ben"otigt eine niedrige Lernrate im Allgemeinen mehr Epochen um das lokale Optimum zu erreichen. \\
Um sich m"oglichst dem globalen Maximum anzun"ahern ist das Momentum (M) hilfreich. Dieses erh"oht die Lernrate, wenn sich der Fehler verringert und senkt diese, wenn der Fehler nicht mehr sinkt. Dies sorgt daf"ur, dass das Netz leichter nicht optimale, lokale Maxima "uberspringt und anschlie{\ss}end mit einer kleineren Lernrate sich dem Hochpunkt des gefundenen Maximums ann"ahert.\\
Auch die Struktur des Netzes (H) kann einen positiven Einfluss auf die Klassifikation haben. Dabei sind Eingabe- und Ausgabeschicht (input-/outputlayer) im Allgemeinen auf festgeschrieben, der inputlayer hat so viele Knoten wie es Features gibt und der outputlayer so viele Knoten wie es Klassen gibt. Die versteckten Schichten (hiddenlayer) dazwischen sind aber variabel, hier ist es "ublich mit einer Schicht zu beginnen und diesem eine Knotenanzahl zwischen den Knotenzahlen des input- und outputlayers zuzuweisen. Danach kann man testen, ob das hinzuf"ugen weiterer hiddenlayer sich positiv auf die Fehlerrate auswirkt.\\
